{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install dataManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "\n",
    "import sklearn\n",
    "from  sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.models import load_model\n",
    "\n",
    "class DataManager:\n",
    "    ROOT_PATH = \"dataset\\WESAD\"\n",
    "    FILE_EXT = \".pkl\"\n",
    "    MODELS_DIR = os.path.join(Path().absolute(), 'src', 'models')\n",
    "    SUBJECTS = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "    BASELINE = 1\n",
    "    STRESS = 2\n",
    "    \n",
    "    FEATURE_KEYS =     ['max',  'min', 'mean', 'range', 'std']\n",
    "    FEATURE_ACC_KEYS = ['maxx', 'maxy', 'maxz', 'mean', 'std']\n",
    "\n",
    "    RAW_SENSOR_VALUES = ['ACC', 'EDA','Temp']\n",
    "    \n",
    "    FEATURES = {'a_mean': [], 'a_std': [], 'a_maxx': [], 'a_maxy': [], 'a_maxz': [],\\\n",
    "                'e_max': [],  'e_min': [], 'e_mean': [], 'e_range': [], 'e_std': [], \\\n",
    "                't_max': [],  't_min': [], 't_mean': [], 't_range': [], 't_std': [] }\n",
    "    STRESS_FEATURES = {'a_mean': [], 'a_std': [], 'a_maxx': [], 'a_maxy': [], 'a_maxz': [],\\\n",
    "                'e_max': [],  'e_min': [], 'e_mean': [], 'e_range': [], 'e_std': [], \\\n",
    "                't_max': [],  't_min': [], 't_mean': [], 't_range': [], 't_std': [] }\n",
    "    \n",
    "    BASELINE_DATA = []\n",
    "    STRESS_DATA = []\n",
    "    \n",
    "    last_saved=''\n",
    "    \n",
    "    def __init__(self, ignore_empatica=True, ignore_additional_signals=True):\n",
    "        self.ignore_empatica = ignore_empatica\n",
    "\n",
    "    def get_subject_path(self, subject):\n",
    "        path = os.path.join(DataManager.ROOT_PATH, 'S'+ str(subject), 'S' + str(subject) + DataManager.FILE_EXT).replace(\"\\\\\", \"/\")\n",
    "        print('Loading data for S'+ str(subject))\n",
    "        print('Path=' + path)\n",
    "        if os.path.isfile(path):\n",
    "            return path\n",
    "        else:\n",
    "            print(path)\n",
    "            raise Exception('Invalid subject: ' + str(subject))\n",
    "\n",
    "    def load(self, subject):\n",
    "        with open(self.get_subject_path(subject), 'rb') as file:\n",
    "            data = pickle.load(file, encoding='latin1')\n",
    "            return self.extract_and_reform(data, subject)\n",
    "    \n",
    "    def load_all(self, subjects=SUBJECTS):\n",
    "        for subject in subjects:\n",
    "            self.load(subject)\n",
    "                \n",
    "    \n",
    "    def extract_and_reform(self, data, subject):     \n",
    "        if self.ignore_empatica:\n",
    "            del data['signal']['wrist']\n",
    "        \n",
    "        baseline_indices = np.nonzero(data['label']==DataManager.BASELINE)[0]   \n",
    "        stress_indices = np.nonzero(data['label']==DataManager.STRESS)[0]\n",
    "        base = dict()\n",
    "        stress = dict()\n",
    "        \n",
    "        for value in DataManager.RAW_SENSOR_VALUES: \n",
    "            base[value] = data['signal']['chest'][value][baseline_indices]\n",
    "            stress[value] = data['signal']['chest'][value][stress_indices]\n",
    "        \n",
    "        DataManager.BASELINE_DATA.append(base)\n",
    "        DataManager.STRESS_DATA.append(stress)\n",
    "        \n",
    "        return base, stress\n",
    "    \n",
    "    def get_stats(self, values, window_size=42000, window_shift=175):\n",
    "        num_features = values.size - window_size      \n",
    "        max_tmp = []\n",
    "        min_tmp = []\n",
    "        mean_tmp = []\n",
    "        dynamic_range_tmp = []\n",
    "        std_tmp = []\n",
    "        for i in range(0, num_features, window_shift):\n",
    "            window = values[i:window_size + i]\n",
    "            max_tmp.append(np.amax(window))\n",
    "            min_tmp.append(np.amin(window))\n",
    "            mean_tmp.append(np.mean(window))\n",
    "            dynamic_range_tmp.append(max_tmp[-1] - min_tmp[-1])\n",
    "            std_tmp.append(np.std(window))\n",
    "\n",
    "        features = {}\n",
    "        features['max'] = max_tmp\n",
    "        features['min'] = min_tmp\n",
    "        features['mean'] = mean_tmp\n",
    "        features['range'] = dynamic_range_tmp\n",
    "        features['std'] = std_tmp\n",
    "        return features\n",
    "\n",
    "    def get_features_for_acc(self, values, window_size=42000, window_shift=175):\n",
    "        num_features = len(values[:,1]) - window_size\n",
    "        maxx_tmp = []\n",
    "        maxy_tmp = []\n",
    "        maxz_tmp = []\n",
    "        mean_tmp = []\n",
    "        std_tmp = []        \n",
    "        for i in range(0, num_features, window_shift):\n",
    "            windowx = values[i:window_size + i, 0]\n",
    "            windowy = values[i:window_size + i, 1]\n",
    "            windowz = values[i:window_size + i, 2]\n",
    "                        \n",
    "            meanx = np.mean(windowx)\n",
    "            meany = np.mean(windowy)\n",
    "            meanz = np.mean(windowz)\n",
    "            mean_tmp.append( (meanx + meany + meanz) )\n",
    "\n",
    "            stdx = np.std(windowx)\n",
    "            stdy = np.std(windowy)\n",
    "            stdz = np.std(windowz)\n",
    "            std_tmp.append( (stdx + stdy + stdz) )\n",
    "            \n",
    "            maxx_tmp.append(np.amax(windowx))\n",
    "            maxy_tmp.append(np.amax(windowy))\n",
    "            maxz_tmp.append(np.amax(windowz))\n",
    "\n",
    "        features = {}\n",
    "        features['mean'] = mean_tmp\n",
    "        features['std'] =  std_tmp\n",
    "        features['maxx'] = maxx_tmp\n",
    "        features['maxy'] = maxy_tmp\n",
    "        features['maxz'] = maxz_tmp\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def compute_features(self, subjects=SUBJECTS, data=BASELINE_DATA, window_size=42000, window_shift=175):\n",
    "        keys = list(DataManager.FEATURES.keys())\n",
    "        print('Computing features..')\n",
    "        for subject in subjects:\n",
    "            print(\"\\tsubject:\", subject)\n",
    "            index = subject - 2\n",
    "            key_index = 0\n",
    "            \n",
    "            acc = self.get_features_for_acc(data[index]['ACC'], window_size, window_shift)\n",
    "            for feature in DataManager.FEATURE_ACC_KEYS:\n",
    "                #print('computed ', len(acc[feature]), 'windows for acc ', feature)\n",
    "                DataManager.FEATURES[keys[key_index]].extend(acc[feature])\n",
    "                key_index = key_index + 1\n",
    "            \n",
    "            eda = self.get_stats(data[index]['EDA'], window_size, window_shift)\n",
    "            for feature in DataManager.FEATURE_KEYS:\n",
    "                #print('computed ', len(eda[feature]), 'windows for eda ', feature)\n",
    "                DataManager.FEATURES[keys[key_index]].extend(eda[feature])\n",
    "                key_index = key_index + 1\n",
    "\n",
    "            temp = self.get_stats(data[index]['Temp'], window_size, window_shift)\n",
    "            for feature in DataManager.FEATURE_KEYS:\n",
    "                #print('computed ', len(temp[feature]), 'windows for temp ', feature)\n",
    "                DataManager.FEATURES[keys[key_index]].extend(temp[feature])\n",
    "                key_index = key_index + 1\n",
    "            \n",
    "        return DataManager.FEATURES\n",
    "\n",
    "    def compute_features_stress(self, subjects=SUBJECTS, data=STRESS_DATA, window_size=42000, window_shift=175):\n",
    "        keys = list(DataManager.STRESS_FEATURES.keys())\n",
    "        print('computing features..')    \n",
    "        for subject in subjects:\n",
    "            print(\"\\tsubject:\", subject)\n",
    "            index = subject - 2\n",
    "            key_index = 0\n",
    "            \n",
    "            acc = self.get_features_for_acc(data[index]['ACC'], window_size, window_shift)\n",
    "            for feature in DataManager.FEATURE_ACC_KEYS:\n",
    "                #print('computed ', len(acc[feature]), 'windows for acc ', feature)\n",
    "                DataManager.STRESS_FEATURES[keys[key_index]].extend(acc[feature])\n",
    "                key_index = key_index + 1\n",
    "            \n",
    "            eda = self.get_stats(data[index]['EDA'], window_size, window_shift)\n",
    "            for feature in DataManager.FEATURE_KEYS:\n",
    "                #print('computed ', len(eda[feature]), 'windows for eda ', feature)\n",
    "                DataManager.STRESS_FEATURES[keys[key_index]].extend(eda[feature])\n",
    "                key_index = key_index + 1\n",
    "\n",
    "            temp = self.get_stats(data[index]['Temp'], window_size, window_shift)\n",
    "            for feature in DataManager.FEATURE_KEYS:\n",
    "                #print('computed ', len(temp[feature]), 'windows for temp ', feature)\n",
    "                DataManager.STRESS_FEATURES[keys[key_index]].extend(temp[feature])\n",
    "                key_index = key_index + 1\n",
    "        return DataManager.STRESS_FEATURES\n",
    "\n",
    "    def get_train_and_test_data(self):\n",
    "        X1 = []\n",
    "        X2 = []\n",
    "        for i in range(0, len(DataManager.FEATURES['a_mean'])):\n",
    "            X1.append([DataManager.FEATURES['a_mean'][i], DataManager.FEATURES['a_std'][i],\\\n",
    "                       DataManager.FEATURES['a_maxx'][i], DataManager.FEATURES['a_maxy'][i],\\\n",
    "                       DataManager.FEATURES['a_maxz'][i], DataManager.FEATURES['e_max'][i],\\\n",
    "                       DataManager.FEATURES['e_min'][i],  DataManager.FEATURES['e_mean'][i],\\\n",
    "                       DataManager.FEATURES['e_range'][i],DataManager.FEATURES['e_std'][i],\\\n",
    "                       DataManager.FEATURES['t_max'][i],  DataManager.FEATURES['t_min'][i],\\\n",
    "                       DataManager.FEATURES['t_mean'][i], DataManager.FEATURES['t_range'][i],\\\n",
    "                       DataManager.FEATURES['t_std'][i]])\n",
    "        \n",
    "        for i in range(0,  len(DataManager.STRESS_FEATURES['a_mean'])):\n",
    "            X2.append([DataManager.STRESS_FEATURES['a_mean'][i], DataManager.STRESS_FEATURES['a_std'][i],\\\n",
    "                       DataManager.STRESS_FEATURES['a_maxx'][i], DataManager.STRESS_FEATURES['a_maxy'][i],\\\n",
    "                       DataManager.STRESS_FEATURES['a_maxz'][i], DataManager.STRESS_FEATURES['e_max'][i],\\\n",
    "                       DataManager.STRESS_FEATURES['e_min'][i], DataManager.STRESS_FEATURES['e_mean'][i],\\\n",
    "                       DataManager.STRESS_FEATURES['e_range'][i], DataManager.STRESS_FEATURES['e_std'][i],\\\n",
    "                       DataManager.STRESS_FEATURES['t_max'][i], DataManager.STRESS_FEATURES['t_min'][i],\\\n",
    "                       DataManager.STRESS_FEATURES['t_mean'][i], DataManager.STRESS_FEATURES['t_range'][i],\\\n",
    "                       DataManager.STRESS_FEATURES['t_std'][i]])                \n",
    "        \n",
    "        y1 = [0] * len(X1)\n",
    "        y2 = [1] * len(X2)  \n",
    "        X = np.concatenate((X1, X2), axis=0)\n",
    "        \n",
    "        y = np.concatenate((y1,y2), axis=0)\n",
    "        X_train, X_test, y_train, y_test = \\\n",
    "            train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "        return (X_train, X_test, y_train, y_test)\n",
    "\n",
    "    def normalize(self, data):\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        return scaler.fit_transform(data)\n",
    "\n",
    "    def scale_data(self, X_train, X_test, y_train, y_test):\n",
    "        print(\"Scaling the data...\")\n",
    "        (X_train, X_test, y_train, y_test) = self.get_train_and_test_data()\n",
    "        X_train = self.normalize(X_train)\n",
    "        X_test = self.normalize(X_test)\n",
    "        X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "        X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "        return (X_train, X_test, y_train, y_test)\n",
    "\n",
    "    def build_model(self):\n",
    "        num_neurons = 15\n",
    "        num_features = 15\n",
    "        \n",
    "        print('Building the LSTM NN...')\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(num_neurons, input_shape=(1, num_features), return_sequences=True))\n",
    "        model.add(LSTM(num_neurons, input_shape=(1, num_features), return_sequences=False))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "        model = self.configure_learning(model)\n",
    "        print(model.summary())\n",
    "        return model\n",
    "\n",
    "    def configure_learning(self, model):\n",
    "        opt = SGD(lr=0.05)\n",
    "        model.compile(loss='binary_crossentropy', optimizer=opt,\\\n",
    "                      metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def train_model(self, model, X_train, X_test, y_train, y_test,\\\n",
    "                    batch_size=32, epochs=5):\n",
    "        print('Training network...')\n",
    "        model.fit(X_train, y_train,\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=epochs,\n",
    "                  validation_data=(X_test, y_test))\n",
    "        score, acc = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "        return (model, score, acc)\n",
    "\n",
    "    def load_model(self, file_name=last_saved):\n",
    "        print(\"Loading model:\", file_name)\n",
    "        file = (os.path.join(DataManager.MODELS_DIR, file_name))\n",
    "        model_from_disc = load_model(file)\n",
    "        return model_from_disc\n",
    "\n",
    "    def save_model(self, model):\n",
    "        now = datetime.datetime.now()\n",
    "        DataManager.last_saved = str(\"model-\" + \\\n",
    "                                     str(now.replace(microsecond=0)) +\\\n",
    "                                     \".h5\").replace(\" \", \"\").replace(\":\", \"_\")\n",
    "        model.save(os.path.join(DataManager.MODELS_DIR, DataManager.last_saved))\n",
    "        print(\"Saved model to disc:\",\\\n",
    "              DataManager.last_saved)\n",
    "        \n",
    "    def get_model_results(self, model, X_train, X_test, y_train, y_test,\\\n",
    "                          batch_size=32):\n",
    "        print('batch_size = ', batch_size)\n",
    "        print('Model results from model.evaluate() test data')\n",
    "        score, acc = model.evaluate(X_test , y_test, batch_size=batch_size)\n",
    "        print('score:', score, 'accuracy:', acc)\n",
    "        \n",
    "        y_pred = model.predict(X_test, batch_size=batch_size, verbose=1)\n",
    "        y_pred[y_pred>0.5] = 1 \n",
    "        y_pred[y_pred<=0.5] = 0 \n",
    "        print(\"_________________________________________________________________\")\n",
    "        print('\\nClassification report from model.predict with test data')\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        print(\"_________________________________________________________________\")\n",
    "        print('\\nConfusion matrix from model.predict with test data')\n",
    "        print(confusion_matrix(y_test, y_pred))\n",
    "        print(\"_________________________________________________________________\")\n",
    "\n",
    "    def create_network(self, epochs=5, batch_size=32):\n",
    "        (X_train, X_test, y_train, y_test) = self.get_train_and_test_data()\n",
    "        (X_train, X_test, y_train, y_test) = \\\n",
    "            self.scale_data(X_train, X_test, y_train, y_test)\n",
    "        model = self.build_model()\n",
    "        (model, score, acc) = self.train_model(model, X_train, X_test, y_train, y_test,\\\n",
    "                                 batch_size, epochs)\n",
    "        self.save_model(model)\n",
    "        return (model, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager = DataManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager.load_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager.compute_features();\n",
    "manager.compute_features_stress();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"We have\", len(manager.SUBJECTS), \" subjects\")\n",
    "\n",
    "for feature in manager.FEATURES.keys():\n",
    "    print(\"there are \", len(manager.FEATURES[feature]), \" values for \", feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = []\n",
    "X2 = []\n",
    "for i in range(0,  len(manager.FEATURES['a_mean'])):\n",
    "    X1.append([manager.FEATURES['a_mean'][i], manager.FEATURES['a_std'][i], manager.FEATURES['a_maxx'][i], manager.FEATURES['a_maxy'][i], manager.FEATURES['a_maxz'][i],\\\n",
    "                  manager.FEATURES['e_max'][i], manager.FEATURES['e_min'][i], manager.FEATURES['e_mean'][i], manager.FEATURES['e_range'][i], manager.FEATURES['e_std'][i],\\\n",
    "                  manager.FEATURES['t_max'][i], manager.FEATURES['t_min'][i], manager.FEATURES['t_mean'][i], manager.FEATURES['t_range'][i], manager.FEATURES['t_std'][i]])\n",
    "print(np.shape(X1))\n",
    "\n",
    "for i in range(0,  len(manager.STRESS_FEATURES['a_mean'])):\n",
    "    X2.append([manager.STRESS_FEATURES['a_mean'][i], manager.STRESS_FEATURES['a_std'][i], manager.STRESS_FEATURES['a_maxx'][i], manager.STRESS_FEATURES['a_maxy'][i], manager.STRESS_FEATURES['a_maxz'][i],\\\n",
    "                  manager.STRESS_FEATURES['e_max'][i], manager.STRESS_FEATURES['e_min'][i], manager.STRESS_FEATURES['e_mean'][i], manager.STRESS_FEATURES['e_range'][i], manager.STRESS_FEATURES['e_std'][i],\\\n",
    "                  manager.STRESS_FEATURES['t_max'][i], manager.STRESS_FEATURES['t_min'][i], manager.STRESS_FEATURES['t_mean'][i], manager.STRESS_FEATURES['t_range'][i], manager.STRESS_FEATURES['t_std'][i]] )\n",
    "print(np.shape(X2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Dense, Embedding, Flatten\n",
    "\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = [0] * len(X1)\n",
    "y2 = [1] * len(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y1))\n",
    "print(len(y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((X1, X2), axis=0)\n",
    "print(np.shape(X))\n",
    "\n",
    "y = np.concatenate((y1,y2), axis=0)\n",
    "print(np.shape(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(X_train))\n",
    "print(np.shape(X_test))\n",
    "print(np.shape(y_train))\n",
    "print(np.shape(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    return scaler.fit_transform(data)\n",
    "X_train = normalize(X_train)\n",
    "X_test = normalize(X_test)\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(X_train))\n",
    "print(np.shape(X_test))\n",
    "print(np.shape(y_train))\n",
    "print(np.shape(y_test))\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_neurons = 15\n",
    "num_features = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(num_neurons, input_shape=(1, num_features), return_sequences=True))\\\n",
    "model.add(LSTM(num_neurons, input_shape=(1, num_features), return_sequences=False))\n",
    "model.add(Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n",
    "\n",
    "print(\"inputs: \" , model.input_shape)\n",
    "print(\"outputs: \", model.output_shape)\n",
    "print(\"actual inputs: \", np.shape(X_train))\n",
    "print(\"actual outputs: \", np.shape(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "opt = RMSprop(learning_rate=0.05)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training LSTM...')\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=5,\n",
    "          validation_data=(X_test, y_test))\n",
    "\n",
    "score, acc = model.evaluate(X_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('score:', score)\n",
    "print('accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as file:\n",
    "    file.write(json)\n",
    "    file.close()\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"LSTM_model.h5\")\n",
    "print(\"Model saved to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "# Load the model of interest\n",
    "json_file = open('model.json', 'r')\n",
    "json = json_file.read()\n",
    "json_file.close()\n",
    "model_from_disc = model_from_json(json)\n",
    "model_from_disc.load_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = RMSprop(lr=0.05)\n",
    "model_from_disc.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "score, acc = model_from_disc.evaluate(X_test , y_test, batch_size=batch_size)\n",
    "print('score:', score)\n",
    "print('accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = model_from_disc.predict(X_test, batch_size=2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[y_pred>0.5] = 1 \n",
    "y_pred[y_pred<=0.5] = 0 \n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
